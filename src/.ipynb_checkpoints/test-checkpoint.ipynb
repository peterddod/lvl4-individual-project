{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from utils.DOSELM import DOSELM\n",
    "from utils.DRRN import DRRN\n",
    "import numpy as np\n",
    "import torch\n",
    "import time as t\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_summary(description, predictions, true_labels):\n",
    "    print(\"Evaluation for: \" + description)\n",
    "    precision = precision_score(predictions, true_labels, average=\"macro\")\n",
    "    recall = recall_score(predictions, true_labels, average=\"macro\")\n",
    "    accuracy = accuracy_score(predictions, true_labels)\n",
    "    f1 = fbeta_score(predictions, true_labels, beta=1, average=\"macro\")\n",
    "    print(\"Classifier '%s' has Acc=%0.3f P=%0.3f R=%0.3f F1=%0.3f\" % (description,accuracy,precision,recall,f1))\n",
    "    print(classification_report(predictions, true_labels, digits=3))\n",
    "    print('\\nConfusion matrix:\\n',confusion_matrix(true_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(X):\n",
    "    return (X - X.mean())/(X.std()*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitdata(x, y, splits=2, first=-1):\n",
    "    length = x.shape[0]\n",
    "\n",
    "    out = []\n",
    "    if first == -1:\n",
    "        splitlen = length // splits\n",
    "        for i in range(splits):\n",
    "            out.append((x[splitlen*i:splitlen*(i+1)], y[splitlen*i:splitlen*(i+1)]))\n",
    "    else:\n",
    "        splits-= 1\n",
    "        split1 = int(length*first)\n",
    "        splitn = (length-split1)//splits\n",
    "        out.append((x[0:split1], y[0:split1]))\n",
    "        for i in range(splits):\n",
    "            a = split1+splitn*i\n",
    "            b = split1+splitn*(i+1)\n",
    "            z = (x[a:b], y[a:b])\n",
    "            out.append(z)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "train_X = scale(np.reshape(train_X.ravel(), (-1,784)))\n",
    "test_X = scale(np.reshape(test_X.ravel(), (-1,784)))\n",
    "\n",
    "y = np.zeros([60000,10])\n",
    "\n",
    "for i in range(len(y)):\n",
    "    for j in range(len(y[i])):\n",
    "        if j == train_y[i]:\n",
    "            y[i,j] = 1\n",
    "        else:\n",
    "            y[i,j] = 0\n",
    "            \n",
    "train_y=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for: ELM\n",
      "Classifier 'ELM' has Acc=0.869 P=0.868 R=0.869 F1=0.867\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.960     0.902     0.930      1043\n",
      "         1.0      0.973     0.914     0.942      1208\n",
      "         2.0      0.808     0.893     0.848       934\n",
      "         3.0      0.835     0.893     0.863       944\n",
      "         4.0      0.889     0.822     0.854      1062\n",
      "         5.0      0.805     0.878     0.840       818\n",
      "         6.0      0.921     0.835     0.876      1056\n",
      "         7.0      0.861     0.879     0.870      1007\n",
      "         8.0      0.821     0.821     0.821       974\n",
      "         9.0      0.806     0.852     0.828       954\n",
      "\n",
      "    accuracy                          0.869     10000\n",
      "   macro avg      0.868     0.869     0.867     10000\n",
      "weighted avg      0.873     0.869     0.870     10000\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 941    0    4    1    2    6   17    1    7    1]\n",
      " [   0 1104    7    2    5    0    7    0   10    0]\n",
      " [  29   19  834   10   23    5   34   23   51    4]\n",
      " [   6   14   28  843    2   30   12   18   39   18]\n",
      " [   1    7    6    1  873    4   26    4   13   47]\n",
      " [   9    9    2   39   22  718   39    8   38    8]\n",
      " [  20    4   11    1   15   15  882    3    4    3]\n",
      " [   9   27   17    6   24    4    7  885    7   42]\n",
      " [  14   16   15   29   14   32   25   11  800   18]\n",
      " [  14    8   10   12   82    4    7   54    5  813]]\n"
     ]
    }
   ],
   "source": [
    "# torch.manual_seed(6519)  # this seed scores really badly!\n",
    "torch.manual_seed(666)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "drrn = DRRN(784, 10, 3000, subnets=1, device=device)\n",
    "\n",
    "X = torch.from_numpy(train_X).float()\n",
    "y = torch.from_numpy(train_y).float()\n",
    "\n",
    "result = drrn.train(X, y)\n",
    "tX = torch.from_numpy(test_X).float()\n",
    "pred = drrn.predict(tX)\n",
    "pred_arg = np.zeros(10000)\n",
    "\n",
    "for i in range(len(pred)):\n",
    "    pred_arg[i] = np.argmax(pred[i])\n",
    "\n",
    "evaluation_summary('ELM', pred_arg, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "doselm = DOSELM(784, 10, [250], device=device)\n",
    "split_data = splitdata(train_X, train_y, 2)\n",
    "\n",
    "train_start = t.time()\n",
    "j = 0\n",
    "\n",
    "for i in split_data:\n",
    "    X = torch.from_numpy(i[0]).float()\n",
    "    y = torch.from_numpy(i[1]).float()\n",
    "    result = doselm.train(X,y)\n",
    "    j+=1\n",
    "    if j==0:\n",
    "        break\n",
    "train_end = t.time()\n",
    "\n",
    "tX = torch.from_numpy(test_X).float()\n",
    "\n",
    "pred_start = t.time()\n",
    "pred = doselm.predict(tX)\n",
    "pred_end = t.time()\n",
    "pred_arg = np.zeros(10000)\n",
    "\n",
    "for i in range(len(pred)):\n",
    "    pred_arg[i] = np.argmax(pred[i])\n",
    "\n",
    "evaluation_summary('ELM', pred_arg, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "doselm = DOSELM(784, 10, [1000], device=device)\n",
    "\n",
    "X = torch.from_numpy(train_X).float()[:60000]\n",
    "y = torch.from_numpy(train_y).float()[:60000]\n",
    "\n",
    "result = doselm.train(X, y)\n",
    "tX = torch.from_numpy(test_X).float()\n",
    "pred = doselm.predict(tX)\n",
    "pred_arg = np.zeros(10000)\n",
    "\n",
    "for i in range(len(pred)):\n",
    "    pred_arg[i] = np.argmax(pred[i])\n",
    "\n",
    "evaluation_summary('ELM', pred_arg, test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
